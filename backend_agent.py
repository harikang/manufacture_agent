"""
Backend Agent for Diecasting AI
FastAPI server with Bedrock Claude Tool Use for Lambda orchestration
Claude directly decides which tools to call and synthesizes the final response
"""
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import json
import os
from typing import Dict, Any, Optional, List
import asyncio
import boto3
from botocore.auth import SigV4Auth
from botocore.awsrequest import AWSRequest
from botocore.credentials import Credentials
import requests

app = FastAPI(title="Diecasting AI Backend Agent")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Lambda URLs (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • í•„ìš”)
USE_MOCK = os.getenv("USE_MOCK", "false").lower() == "true"
LAMBDA_T0_URL = os.getenv("LAMBDA_T0_URL", "https://your-lambda-t0-url.lambda-url.us-east-1.on.aws/")
LAMBDA_T1_URL = os.getenv("LAMBDA_T1_URL", "https://your-lambda-t1-url.lambda-url.us-east-1.on.aws/")
LAMBDA_T2_URL = os.getenv("LAMBDA_T2_URL", "https://your-lambda-t2-url.lambda-url.us-east-1.on.aws/")
LAMBDA_T3_URL = os.getenv("LAMBDA_T3_URL", "https://your-lambda-t3-url.lambda-url.us-east-1.on.aws/")

# Bedrock Client
bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')
MODEL_ID = os.getenv("MODEL_ID", "us.anthropic.claude-sonnet-4-5-20250929-v1:0")

# AWS Session for SigV4 signing
session = boto3.Session(region_name='us-east-1')
credentials = session.get_credentials()

# In-Memory Conversation Storage
conversation_memory: Dict[str, List[Dict[str, Any]]] = {}
MAX_HISTORY_LENGTH = 10  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€

def sign_request(url: str, method: str, payload: dict) -> dict:
    """Sign request with SigV4 for Lambda Function URL"""
    body = json.dumps(payload)
    request = AWSRequest(method=method, url=url, data=body, headers={
        'Content-Type': 'application/json'
    })
    SigV4Auth(credentials, 'lambda', 'us-east-1').add_auth(request)
    return dict(request.headers)

# Request Models
class PredictRequest(BaseModel):
    question: str
    features: Dict[str, float]
    session_id: Optional[str] = "default"

class SessionRequest(BaseModel):
    user_id: Optional[str] = "anonymous"

class KBIngestRequest(BaseModel):
    action: str
    job_id: Optional[str] = None

# =============================================================================
# Tool Definitions for Bedrock Claude
# =============================================================================
TOOLS = [
    {
        "name": "predict_quality",
        "description": """í’ˆì§ˆ ì˜ˆì¸¡ ë„êµ¬. í˜„ì¬ ê³µì • íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆì´ ì–‘í’ˆ/ë¶ˆëŸ‰ì¸ì§€ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
ì‚¬ìš© ì‹œì :
- ì‚¬ìš©ìê°€ í˜„ì¬ ì¡°ê±´ì—ì„œ í’ˆì§ˆì„ ì˜ˆì¸¡í•´ë‹¬ë¼ê³  í•  ë•Œ
- ë¶ˆëŸ‰ ê°€ëŠ¥ì„±, ì–‘í’ˆ í™•ë¥ ì„ ë¬¼ì–´ë³¼ ë•Œ
- "ì˜ˆì¸¡í•´ì¤˜", "ë¶ˆëŸ‰ì¼ê¹Œ?", "í’ˆì§ˆ íŒì •" ë“±ì˜ ì§ˆë¬¸""",
        "inputSchema": {
            "json": {
                "type": "object",
                "properties": {
                    "features": {
                        "type": "object",
                        "description": "ê³µì • íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ (ì„¼ì„œê°’ë“¤)"
                    }
                },
                "required": ["features"]
            }
        }
    },
    {
        "name": "analyze_feature_importance",
        "description": """Feature Importance ë¶„ì„ ë„êµ¬. í’ˆì§ˆì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì£¼ìš” ë³€ìˆ˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
ì‚¬ìš© ì‹œì :
- ì–´ë–¤ ë³€ìˆ˜ê°€ í’ˆì§ˆì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¬¼ì–´ë³¼ ë•Œ
- ë¶ˆëŸ‰ ì›ì¸ì„ ë¶„ì„í•´ë‹¬ë¼ê³  í•  ë•Œ
- "ì™œ ë¶ˆëŸ‰ì´ì•¼?", "ì˜í–¥ ìš”ì¸", "ì¤‘ìš” ë³€ìˆ˜" ë“±ì˜ ì§ˆë¬¸
ì£¼ì˜: ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ predict_qualityë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.""",
        "inputSchema": {
            "json": {
                "type": "object",
                "properties": {
                    "features": {
                        "type": "object",
                        "description": "ê³µì • íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬"
                    },
                    "latent_features": {
                        "type": "array",
                        "items": {"type": "number"},
                        "description": "predict_qualityì—ì„œ ë°˜í™˜ëœ latent_features"
                    }
                },
                "required": ["features", "latent_features"]
            }
        }
    },
    {
        "name": "search_knowledge_base",
        "description": """ê³µì • ì§€ì‹ ê²€ìƒ‰ ë„êµ¬. Knowledge Baseì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
ì‚¬ìš© ì‹œì :
- ì¥ë¹„ ìŠ¤í™, ê¶Œì¥ ë²”ìœ„ë¥¼ ë¬¼ì–´ë³¼ ë•Œ
- SOP, ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ì°¾ì„ ë•Œ
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œê°€ í•„ìš”í•  ë•Œ
- "ê¶Œì¥ ë²”ìœ„", "ìŠ¤í™", "ì–´ë–»ê²Œ í•´ê²°", "ë°©ë²•" ë“±ì˜ ì§ˆë¬¸""",
        "inputSchema": {
            "json": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  ì§ˆë¬¸ ë˜ëŠ” í‚¤ì›Œë“œ"
                    }
                },
                "required": ["query"]
            }
        }
    }
]

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë‹¤ì´ìºìŠ¤íŒ… ì œì¡° ê³µì • AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ì ì ˆí•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

## ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ

1. **í’ˆì§ˆ ì˜ˆì¸¡ ì§ˆë¬¸** â†’ predict_quality í˜¸ì¶œ
   - "ë¶ˆëŸ‰ ê°€ëŠ¥ì„±ì€?", "í’ˆì§ˆ ì˜ˆì¸¡í•´ì¤˜"

2. **ì›ì¸ ë¶„ì„ ì§ˆë¬¸** â†’ predict_quality ë¨¼ì € í˜¸ì¶œ â†’ ê²°ê³¼ í™•ì¸ í›„ â†’ analyze_feature_importance í˜¸ì¶œ
   - "ì™œ ë¶ˆëŸ‰ì´ì•¼?", "ì–´ë–¤ ë³€ìˆ˜ê°€ ì˜í–¥ì„ ë¯¸ì³?"
   - **ì¤‘ìš”: ë°˜ë“œì‹œ predict_qualityë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ê³ , ê·¸ ê²°ê³¼ì˜ latent_featuresë¥¼ ë°›ì€ í›„ì— analyze_feature_importanceë¥¼ í˜¸ì¶œí•˜ì„¸ìš”**
   - **ë‘ ë„êµ¬ë¥¼ ë™ì‹œì— í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.**

3. **ê³µì • ì§€ì‹ ì§ˆë¬¸** â†’ search_knowledge_base í˜¸ì¶œ
   - "ê¶Œì¥ ë²”ìœ„", "ìŠ¤í™", "í•´ê²° ë°©ë²•"

## ì¤‘ìš” ê·œì¹™
- **ê° ë„êµ¬ëŠ” í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì„¸ìš”. ê°™ì€ ë„êµ¬ë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.**
- **search_knowledge_base ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°›ìœ¼ë©´, ì¦‰ì‹œ ë‹µë³€ì„ ì¢…ë£Œí•˜ì„¸ìš”. ì ˆëŒ€ ì¶”ê°€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.**
- analyze_feature_importanceëŠ” ë°˜ë“œì‹œ predict_quality í˜¸ì¶œ í›„ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- predict_qualityì˜ ì‘ë‹µì—ì„œ latent_features ë°°ì—´ì„ ì¶”ì¶œí•˜ì—¬ analyze_feature_importanceì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤
- í•œ ë²ˆì— í•˜ë‚˜ì˜ ë„êµ¬ë§Œ í˜¸ì¶œí•˜ì„¸ìš” (ë³‘ë ¬ í˜¸ì¶œ ê¸ˆì§€)

## ì‘ë‹µ ê°€ì´ë“œ
- **search_knowledge_base ê²°ê³¼ë¥¼ ë°›ìœ¼ë©´:**
  - answer í•„ë“œì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•˜ì„¸ìš”
  - ì ˆëŒ€ "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ê°™ì€ ë§ì„ í•˜ì§€ ë§ˆì„¸ìš”
  - answer í•„ë“œì— ì´ë¯¸ ì™„ì „í•œ ë‹µë³€ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤
  - ë‹¤ë¥¸ ë„êµ¬ë¥¼ ì¶”ê°€ë¡œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”
- ì˜ˆì¸¡ ê²°ê³¼ëŠ” probability_percent í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ %ë¡œ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: "ë¶ˆëŸ‰ í™•ë¥  75.0%")
- Feature ImportanceëŠ” top_features_percent í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ %ë¡œ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: "Temperature1: 15.23%")
- ìƒìœ„ 5ê°œ ë³€ìˆ˜ë¥¼ ê°•ì¡°í•˜ì„¸ìš”
- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”"""

# =============================================================================
# Tool Result Summarizer (LLMì— ì „ë‹¬í•  ë•Œ í† í° ì ˆì•½)
# =============================================================================
def summarize_tool_result(tool_name: str, result: Dict[str, Any]) -> Dict[str, Any]:
    """ë„êµ¬ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ LLMì— ì „ë‹¬ (ë¶ˆí•„ìš”í•œ ë°ì´í„° ì œê±°)"""
    if tool_name == "predict_quality":
        # latent_featuresëŠ” ìƒìœ„ 24ê°œë§Œ ì „ë‹¬``
        latent = result.get("latent_features", [])[:24]
        return {
            "prediction": result.get("prediction"),
            "latent_features": latent
        }
    elif tool_name == "analyze_feature_importance":
        # ìƒìœ„ 10ê°œ featureë§Œ ì „ë‹¬, equipment_descriptions ì œê±°
        top_features = result.get("top_features", [])[:10]
        return {
            "top_features": top_features,
            "top_features_percent": result.get("top_features_percent", [])[:10]
        }
    elif tool_name == "search_knowledge_base":
        # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì „ë‹¬
        results = result.get("results", [])[:3]
        return {"results": results}
    
    return result

# =============================================================================
# Lambda Callers (ë™ê¸° ë²„ì „ - Tool Useì—ì„œ ì‚¬ìš©) - SigV4 ì„œëª… ì ìš©
# =============================================================================
def call_lambda_t1_sync(features: Dict[str, float]) -> Dict[str, Any]:
    """í’ˆì§ˆ ì˜ˆì¸¡ Lambda í˜¸ì¶œ (IAM ì¸ì¦)"""
    if USE_MOCK:
        import random
        defect_prob = 0.75 if random.random() > 0.5 else 0.25
        return {
            "prediction": {
                "class": "defect" if defect_prob > 0.5 else "normal",
                "probability": defect_prob,
                "class_probabilities": {"normal": 1-defect_prob, "defect": defect_prob}
            },
            "latent_features": [random.random()*2-1 for _ in range(12)]
        }
    
    try:
        payload = {"features": features}
        headers = sign_request(LAMBDA_T1_URL, 'POST', payload)
        response = requests.post(
            LAMBDA_T1_URL,
            json=payload,
            headers=headers,
            timeout=30
        )
        data = response.json()
        body = data.get("body", data)
        if isinstance(body, str):
            body = json.loads(body)
        return body
    except Exception as e:
        return {"error": str(e)}

def call_lambda_t2_sync(features: Dict[str, float], latent: List[float]) -> Dict[str, Any]:
    """Feature Importance Lambda í˜¸ì¶œ (IAM ì¸ì¦)"""
    if USE_MOCK:
        import random
        feature_names = list(features.keys())
        random.shuffle(feature_names)
        return {
            "top_features": [[name, 0.15 - i*0.012] for i, name in enumerate(feature_names[:10])]
        }
    
    try:
        payload = {"features": features, "latent_features": latent, "top_n": 10}
        headers = sign_request(LAMBDA_T2_URL, 'POST', payload)
        response = requests.post(
            LAMBDA_T2_URL,
            json=payload,
            headers=headers,
            timeout=30
        )
        data = response.json()
        body = data.get("body", data)
        if isinstance(body, str):
            body = json.loads(body)
        return body
    except Exception as e:
        return {"error": str(e)}

def call_lambda_t3_sync(query: str) -> Dict[str, Any]:
    """Knowledge Base ê²€ìƒ‰ Lambda í˜¸ì¶œ (IAM ì¸ì¦)"""
    if USE_MOCK:
        return {
            "answer": f"'{query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\në‹¤ì´ìºìŠ¤íŒ… ê³µì •ì—ì„œ í•´ë‹¹ íŒŒë¼ë¯¸í„°ì˜ ê¶Œì¥ ë²”ìœ„ì™€ ê´€ë¦¬ ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…ë“œë¦½ë‹ˆë‹¤.",
            "sources": [{"title": "ë‹¤ì´ìºìŠ¤íŒ… ê³µì • ê°€ì´ë“œ", "type": "Knowledge Base"}]
        }
    
    try:
        payload = {"query": query}
        headers = sign_request(LAMBDA_T3_URL, 'POST', payload)
        response = requests.post(
            LAMBDA_T3_URL,
            json=payload,
            headers=headers,
            timeout=30
        )
        print(f"ğŸ” Lambda T3 Response Status: {response.status_code}")
        print(f"ğŸ” Lambda T3 Raw Response: {response.text[:500]}")
        
        data = response.json()
        body = data.get("body", data)
        if isinstance(body, str):
            body = json.loads(body)
        
        print(f"ğŸ” Lambda T3 Parsed Body: {json.dumps(body, ensure_ascii=False)[:500]}")
        return body
    except Exception as e:
        print(f"âŒ Lambda T3 Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}

# =============================================================================
# Tool Execution
# =============================================================================
def execute_tool(tool_name: str, tool_input: Dict[str, Any], features: Dict[str, float]) -> Dict[str, Any]:
    """ë„êµ¬ ì‹¤í–‰"""
    if tool_name == "predict_quality":
        # featuresëŠ” tool_inputì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜, ìš”ì²­ì˜ features ì‚¬ìš©
        feat = tool_input.get("features", features)
        result = call_lambda_t1_sync(feat)
        
        # í™•ë¥ ì„ %ë¡œ ë³€í™˜ (NaN ë°©ì§€)
        if "prediction" in result:
            pred = result["prediction"]
            if "probability" in pred and isinstance(pred["probability"], (int, float)):
                pred["probability_percent"] = f"{pred['probability'] * 100:.1f}%"
            else:
                pred["probability_percent"] = "0.0%"
            
            if "class_probabilities" in pred and isinstance(pred["class_probabilities"], dict):
                pred["class_probabilities_percent"] = {
                    k: f"{v * 100:.1f}%" if isinstance(v, (int, float)) else "0.0%"
                    for k, v in pred["class_probabilities"].items()
                }
        return result
    
    elif tool_name == "analyze_feature_importance":
        feat = tool_input.get("features", features)
        latent = tool_input.get("latent_features", [])
        result = call_lambda_t2_sync(feat, latent)
        
        # Feature Importanceë¥¼ %ë¡œ ë³€í™˜ (NaN ë°©ì§€)
        if "top_features" in result and isinstance(result["top_features"], list):
            result["top_features_percent"] = [
                [name, f"{importance * 100:.2f}%" if isinstance(importance, (int, float)) else "0.00%"]
                for name, importance in result["top_features"]
            ]
        return result
    
    elif tool_name == "search_knowledge_base":
        query = tool_input.get("query", "")
        return call_lambda_t3_sync(query)
    
    return {"error": f"Unknown tool: {tool_name}"}

# =============================================================================
# Bedrock Converse API with Tool Use
# =============================================================================

def run_agent_loop(question: str, features: Dict[str, float], session_id: str = "default"):
    """
    Bedrock Claude Tool Use ê¸°ë°˜ ì—ì´ì „íŠ¸ ë£¨í”„
    Claudeê°€ ë„êµ¬ í˜¸ì¶œì„ ê²°ì •í•˜ê³ , ê²°ê³¼ë¥¼ ë°›ì•„ ìµœì¢… ì‘ë‹µ ìƒì„±
    """
    # ì„¸ì…˜ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    if session_id not in conversation_memory:
        conversation_memory[session_id] = []
    
    # ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    history = conversation_memory[session_id]
    
    # ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages = []
    
    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìµœê·¼ MAX_HISTORY_LENGTHê°œë§Œ)
    for msg in history[-MAX_HISTORY_LENGTH:]:
        messages.append(msg)
    
    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    current_message = {
        "role": "user",
        "content": [
            {
                "text": f"""ì‚¬ìš©ì ì§ˆë¬¸: {question}

í˜„ì¬ ê³µì • íŒŒë¼ë¯¸í„°:
{json.dumps(features, indent=2, ensure_ascii=False)}

ìœ„ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."""
            }
        ]
    }
    messages.append(current_message)
    
    tool_results = []  # ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ ì €ì¥
    
    # Agent Loop - ìµœëŒ€ 5íšŒ ë°˜ë³µ
    for iteration in range(5):
        # Bedrock Converse API í˜¸ì¶œ
        response = bedrock_runtime.converse(
            modelId=MODEL_ID,
            system=[{"text": SYSTEM_PROMPT}],
            messages=messages,
            toolConfig={
                "tools": [
                    {"toolSpec": tool} for tool in TOOLS
                ]
            }
        )
        
        output = response.get("output", {})
        message = output.get("message", {})
        stop_reason = response.get("stopReason", "")
        
        # ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ëŒ€í™”ì— ì¶”ê°€
        messages.append(message)
        
        # ì¢…ë£Œ ì¡°ê±´: end_turnì´ë©´ ìµœì¢… ì‘ë‹µ
        if stop_reason == "end_turn":
            # í…ìŠ¤íŠ¸ ì‘ë‹µ ì¶”ì¶œ
            final_text = ""
            for content in message.get("content", []):
                if "text" in content:
                    final_text = content["text"]
                    break
            
            return {
                "answer": final_text,
                "tool_results": tool_results
            }
        
        # Tool Use ì²˜ë¦¬
        if stop_reason == "tool_use":
            tool_use_results = []
            should_return_immediately = False
            immediate_return_data = None
            
            for content in message.get("content", []):
                if "toolUse" in content:
                    tool_use = content["toolUse"]
                    tool_name = tool_use["name"]
                    tool_input = tool_use.get("input", {})
                    tool_use_id = tool_use["toolUseId"]
                    
                    # ë„êµ¬ ì‹¤í–‰
                    result = execute_tool(tool_name, tool_input, features)
                    tool_results.append({
                        "tool": tool_name,
                        "input": tool_input,
                        "result": result
                    })
                    
                    # search_knowledge_base í˜¸ì¶œ ì‹œ ì¦‰ì‹œ ë‹µë³€ ë°˜í™˜ í”Œë˜ê·¸ ì„¤ì •
                    if tool_name == "search_knowledge_base" and "answer" in result:
                        should_return_immediately = True
                        immediate_return_data = {
                            "answer": result["answer"],
                            "tool_results": tool_results,
                            "sources": result.get("sources", [])
                        }
                        break  # ë” ì´ìƒ ë„êµ¬ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
                    
                    # Tool Result ë©”ì‹œì§€ êµ¬ì„±
                    tool_use_results.append({
                        "toolUseId": tool_use_id,
                        "content": [{"json": result}]
                    })
            
            # search_knowledge_base ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
            if should_return_immediately:
                return immediate_return_data
            
            # Tool Resultë¥¼ ëŒ€í™”ì— ì¶”ê°€
            if tool_use_results:
                messages.append({
                    "role": "user",
                    "content": [{"toolResult": tr} for tr in tool_use_results]
                })
    
    # ëŒ€í™” ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ì— ì €ì¥
    # ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ìµœì¢… ì‘ë‹µë§Œ ì €ì¥
    conversation_memory[session_id].append(current_message)
    if messages:
        last_assistant_message = messages[-1]
        if last_assistant_message.get("role") == "assistant":
            conversation_memory[session_id].append(last_assistant_message)
    
    # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
    if len(conversation_memory[session_id]) > MAX_HISTORY_LENGTH * 2:
        conversation_memory[session_id] = conversation_memory[session_id][-MAX_HISTORY_LENGTH * 2:]
    
    return {
        "answer": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "tool_results": tool_results
    }

# =============================================================================
# SSE Streaming with Real-time Agent Loop
# =============================================================================
async def agent_event_stream(question: str, features: Dict[str, float], session_id: str = "default"):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì‹¤ì‹œê°„ SSEë¡œ ìŠ¤íŠ¸ë¦¬ë° - ê° ë‹¨ê³„ë³„ë¡œ ì¦‰ì‹œ ì „ì†¡"""
    import time
    start_time = time.time()

    def elapsed():
        return round(time.time() - start_time, 2)

    try:
        # ì„¸ì…˜ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if session_id not in conversation_memory:
            conversation_memory[session_id] = []
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        history = conversation_memory[session_id]
        
        # 1. ë¶„ì„ ì‹œì‘ - ëª…í™•í•œ ì´ë²¤íŠ¸ ì „ì†¡
        yield f"data: {json.dumps({'type': 'status', 'message': 'AI ì—ì´ì „íŠ¸ ì‹œì‘', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.05)
        
        yield f"data: {json.dumps({'type': 'thinking', 'message': 'ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.05)

        # 2. ì—ì´ì „íŠ¸ ë£¨í”„ ì‹¤í–‰ - Generator ë²„ì „
        messages = []
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìµœê·¼ MAX_HISTORY_LENGTHê°œë§Œ)
        for msg in history[-MAX_HISTORY_LENGTH:]:
            messages.append(msg)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€ (ê°„ê²°í•˜ê²Œ ì €ì¥)
        current_message = {
            "role": "user",
            "content": [{"text": question}]
        }
        
        # API í˜¸ì¶œìš© ë©”ì‹œì§€ (ìƒì„¸ ì •ë³´ í¬í•¨)
        api_message = {
            "role": "user",
            "content": [
                {
                    "text": f"""ì‚¬ìš©ì ì§ˆë¬¸: {question}

í˜„ì¬ ê³µì • íŒŒë¼ë¯¸í„°:
{json.dumps(features, indent=2, ensure_ascii=False)}

ìœ„ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."""
                }
            ]
        }
        messages.append(api_message)

        tool_results_data = []  # ìµœì¢… ê²°ê³¼ ì €ì¥

        # Agent Loop - ìµœëŒ€ 3íšŒ ë°˜ë³µ (ë„êµ¬ í˜¸ì¶œ â†’ ìµœì¢… ë‹µë³€)
        for iteration in range(3):
            # ë„êµ¬ í˜¸ì¶œ í›„ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ì„ì„ í‘œì‹œ
            if iteration > 0 and tool_use_results:
                yield f"data: {json.dumps({'type': 'status', 'message': 'ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
            else:
                yield f"data: {json.dumps({'type': 'status', 'message': f'ë„êµ¬ ì„ íƒ ì¤‘... (ë°˜ë³µ {iteration + 1})', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.05)

            # Bedrock Converse API í˜¸ì¶œ (ë™ê¸°)
            llm_start = time.time()
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: bedrock_runtime.converse(
                    modelId=MODEL_ID,
                    system=[{"text": SYSTEM_PROMPT}],
                    messages=messages,
                    toolConfig={
                        "tools": [{"toolSpec": tool} for tool in TOOLS]
                    }
                )
            )
            llm_duration = time.time() - llm_start
            print(f"â±ï¸ LLM í˜¸ì¶œ ì‹œê°„: {llm_duration:.2f}ì´ˆ (iteration {iteration + 1})")

            output = response.get("output", {})
            message = output.get("message", {})
            stop_reason = response.get("stopReason", "")

            # ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ëŒ€í™”ì— ì¶”ê°€
            messages.append(message)

            # ì¢…ë£Œ ì¡°ê±´: end_turnì´ë©´ ìµœì¢… ì‘ë‹µ
            if stop_reason == "end_turn":
                # í…ìŠ¤íŠ¸ ì‘ë‹µ ì¶”ì¶œ
                final_text = ""
                for content in message.get("content", []):
                    if "text" in content:
                        final_text = content["text"]
                        break

                yield f"data: {json.dumps({'type': 'ai_response', 'data': {'answer': final_text}, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.2)  # ì¶©ë¶„í•œ ì‹œê°„ í™•ë³´
                yield f"data: {json.dumps({'type': 'done', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.1)  # done ì´ë²¤íŠ¸ ì „ì†¡ í™•ì¸
                
                # ëŒ€í™” ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ê°„ê²°í•œ ë²„ì „ë§Œ)
                conversation_memory[session_id].append(current_message)
                # Assistant ì‘ë‹µë„ ê°„ê²°í•˜ê²Œ ì €ì¥
                assistant_message = {
                    "role": "assistant",
                    "content": [{"text": final_text}]
                }
                conversation_memory[session_id].append(assistant_message)
                
                # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
                if len(conversation_memory[session_id]) > MAX_HISTORY_LENGTH * 2:
                    conversation_memory[session_id] = conversation_memory[session_id][-MAX_HISTORY_LENGTH * 2:]
                
                return

            # Tool Use ì²˜ë¦¬
            if stop_reason == "tool_use":
                tool_use_results = []
                kb_answer = None  # KB ê²€ìƒ‰ ê²°ê³¼ ì €ì¥

                for content in message.get("content", []):
                    if "toolUse" in content:
                        tool_use = content["toolUse"]
                        tool_name = tool_use["name"]
                        tool_input = tool_use.get("input", {})
                        tool_use_id = tool_use["toolUseId"]

                        # â˜… Tool í˜¸ì¶œ ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡
                        yield f"data: {json.dumps({'type': 'tool_start', 'tool': tool_name, 'input': tool_input, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0.1)  # ì´ë²¤íŠ¸ í™•ì‹¤íˆ ì „ì†¡

                        # ë„êµ¬ ì‹¤í–‰ (ë™ê¸°)
                        result = await loop.run_in_executor(
                            None,
                            execute_tool,
                            tool_name,
                            tool_input,
                            features
                        )

                        # â˜… Tool í˜¸ì¶œ ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
                        yield f"data: {json.dumps({'type': 'tool_end', 'tool': tool_name, 'result': result, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0.1)

                        # â˜… ê²°ê³¼ íƒ€ì…ë³„ ì´ë²¤íŠ¸ ì „ì†¡ (UIì—ì„œ ì‹¤ì‹œê°„ ë Œë”ë§)
                        if tool_name == "predict_quality":
                            yield f"data: {json.dumps({'type': 't1_result', 'data': result, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                        elif tool_name == "analyze_feature_importance":
                            yield f"data: {json.dumps({'type': 't2_result', 'data': result, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                        elif tool_name == "search_knowledge_base":
                            yield f"data: {json.dumps({'type': 't3_result', 'data': result, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                            # KB ê²€ìƒ‰ ê²°ê³¼ëŠ” ì¦‰ì‹œ ë°˜í™˜
                            if "answer" in result:
                                kb_answer = result["answer"]
                        
                        await asyncio.sleep(0.1)

                        tool_results_data.append({
                            "tool": tool_name,
                            "input": tool_input,
                            "result": result
                        })

                        # Tool Resultë¥¼ ìš”ì•½í•´ì„œ LLMì— ì „ë‹¬ (í† í° ì ˆì•½)
                        summarized_result = summarize_tool_result(tool_name, result)
                        
                        # Tool Result ë©”ì‹œì§€ êµ¬ì„±
                        tool_use_results.append({
                            "toolUseId": tool_use_id,
                            "content": [{"json": summarized_result}]
                        })

                        await asyncio.sleep(0.1)  # SSE ë²„í¼ í”ŒëŸ¬ì‹œ

                # KB ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (ì¶”ê°€ LLM í˜¸ì¶œ ì—†ì´)
                if kb_answer:
                    yield f"data: {json.dumps({'type': 'ai_response', 'data': {'answer': kb_answer}, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.2)
                    yield f"data: {json.dumps({'type': 'done', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                    
                    # ë©”ëª¨ë¦¬ì— ì €ì¥
                    conversation_memory[session_id].append(current_message)
                    assistant_message = {
                        "role": "assistant",
                        "content": [{"text": kb_answer}]
                    }
                    conversation_memory[session_id].append(assistant_message)
                    
                    if len(conversation_memory[session_id]) > MAX_HISTORY_LENGTH * 2:
                        conversation_memory[session_id] = conversation_memory[session_id][-MAX_HISTORY_LENGTH * 2:]
                    
                    return

                # Tool Resultë¥¼ ëŒ€í™”ì— ì¶”ê°€
                if tool_use_results:
                    messages.append({
                        "role": "user",
                        "content": [{"toolResult": tr} for tr in tool_use_results]
                    })
                    
                    # ë‹¤ìŒ ë°˜ë³µ ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
                    yield f"data: {json.dumps({'type': 'status', 'message': 'ë„êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)

        # ëŒ€í™” ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ë£¨í”„ ì¢…ë£Œ ì‹œ)
        conversation_memory[session_id].append(current_message)
        if messages and len(messages) > 0:
            # ë§ˆì§€ë§‰ assistant ë©”ì‹œì§€ ì°¾ê¸°
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•´ì„œ ì €ì¥
                    final_text = ""
                    for content in msg.get("content", []):
                        if "text" in content:
                            final_text = content["text"]
                            break
                    if final_text:
                        assistant_message = {
                            "role": "assistant",
                            "content": [{"text": final_text}]
                        }
                        conversation_memory[session_id].append(assistant_message)
                    break
        
        # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
        if len(conversation_memory[session_id]) > MAX_HISTORY_LENGTH * 2:
            conversation_memory[session_id] = conversation_memory[session_id][-MAX_HISTORY_LENGTH * 2:]
        
        # ìµœëŒ€ ë°˜ë³µ ì´ˆê³¼
        yield f"data: {json.dumps({'type': 'ai_response', 'data': {'answer': 'ì²˜ë¦¬ ì¤‘ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.'}, 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.05)
        yield f"data: {json.dumps({'type': 'done', 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"Agent Error: {error_detail}")
        yield f"data: {json.dumps({'type': 'error', 'message': str(e), 'elapsed': elapsed()}, ensure_ascii=False)}\n\n"

# =============================================================================
# API Endpoints
# =============================================================================
@app.post("/api/sessions")
async def create_session(req: SessionRequest):
    """ì„¸ì…˜ ìƒì„±"""
    return {
        "session_id": f"sess_{req.user_id}_{os.urandom(4).hex()}",
        "created_at": "2026-02-03T15:00:00Z"
    }

@app.post("/api/chat")
async def chat(req: PredictRequest):
    """ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - Bedrock Tool Use ê¸°ë°˜"""
    return StreamingResponse(
        agent_event_stream(req.question, req.features, req.session_id),
        media_type="text/event-stream"
    )

@app.get("/health")
async def health():
    return {"status": "healthy", "agent": "bedrock-tool-use"}

# KB Ingest (SigV4 ì„œëª… ì ìš©)
async def call_lambda_t0(action: str, job_id: Optional[str] = None) -> Dict[str, Any]:
    try:
        payload = {"body": {"action": action}}
        if job_id:
            payload["body"]["job_id"] = job_id
        
        headers = sign_request(LAMBDA_T0_URL, 'POST', payload)
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(LAMBDA_T0_URL, json=payload, headers=headers)
            data = response.json()
            body = data.get("body", data)
            if isinstance(body, str):
                body = json.loads(body)
            return body
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/kb-ingest")
async def kb_ingest(req: KBIngestRequest):
    """KB ì¸ì œìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    result = await call_lambda_t0(req.action, req.job_id)
    return result

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
