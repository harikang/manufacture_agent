# ë‹¤ì´ìºìŠ¤íŒ… í’ˆì§ˆ ì˜ˆì¸¡ ëª¨ë¸ ìš”ì•½

## ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì„±

ë³¸ ì‹œìŠ¤í…œì€ **AutoEncoder ê¸°ë°˜ Feature Extraction + Gradient Boosting ë¶„ë¥˜**ì˜ 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```
ì…ë ¥ (30D features)
    â†“
[Stage 1] AutoEncoder Feature Extraction
    â†“
Latent Features (12D)
    â†“
Combined Features (30D + 12D = 42D)
    â†“
[Stage 2] Gradient Boosting Classifier
    â†“
ì˜ˆì¸¡ ê²°ê³¼ (ì •ìƒ/ë¶ˆëŸ‰)
```

### ìµœì¢… ì„±ëŠ¥
- **F1-Score**: 0.7027
- **ROC-AUC**: 0.9175
- **Accuracy**: 0.8832
- **Precision**: 0.82
- **Recall**: 0.58

---

## Stage 1: AutoEncoder (Feature Extraction)

### ì—­í• 
- 30D ì›ë³¸ featuresë¥¼ 12D latent featuresë¡œ ì••ì¶•
- ì¤‘ìš”í•œ íŒ¨í„´ê³¼ íŠ¹ì§•ì„ í•™ìŠµí•˜ì—¬ ì°¨ì› ì¶•ì†Œ
- Gradient Boostingì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ê³ í’ˆì§ˆ features ìƒì„±

### í•µì‹¬ êµ¬ì„± ìš”ì†Œ
1. **SwiGLU í™œì„±í™” í•¨ìˆ˜**: Swish-Gated Linear Unitìœ¼ë¡œ ë” ë‚˜ì€ í‘œí˜„ë ¥ ì œê³µ
2. **Attention Module**: 4-head self-attentionìœ¼ë¡œ feature ì¤‘ìš”ë„ í•™ìŠµ
3. **Focal Loss**: ë¶ˆê· í˜• ë°ì´í„°(ë¶ˆëŸ‰ë¥  22.42%)ì— ìµœì í™”ëœ ì†ì‹¤ í•¨ìˆ˜
4. **AdamW Optimizer**: lr=1e-3, weight_decay=1e-4, betas=(0.9, 0.999)
5. **CosineAnnealingWarmRestarts Scheduler**: 
   - T_0=10 (ì´ˆê¸° ì¬ì‹œì‘ ì£¼ê¸°)
   - T_mult=2 (ì¬ì‹œì‘ë§ˆë‹¤ ì£¼ê¸° 2ë°° ì¦ê°€)
   - eta_min=1e-6 (ìµœì†Œ í•™ìŠµë¥ )
   - ë¶ˆê· í˜• ë°ì´í„°ì— ìµœì í™”ëœ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
```
Input (30 features)
    â†“
Encoder: 30 â†’ 64 â†’ 32 â†’ 16 (SwiGLU + BatchNorm + Dropout)
    â†“
Attention Module (4 heads)
    â†“
Latent Space (12 dimensions) â† ì´ ë¶€ë¶„ì´ Gradient Boosting ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë¨
    â†“
Decoder: 12 â†’ 16 â†’ 32 â†’ 64 (SwiGLU + BatchNorm + Dropout)
    â†“
Reconstruction (30 features)
```

**ì´ íŒŒë¼ë¯¸í„° ìˆ˜**: 18,567ê°œ

**ì£¼ì˜**: AutoEncoderì˜ Classifier headëŠ” í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©ë˜ë©°, 
ìµœì¢… ì˜ˆì¸¡ì—ì„œëŠ” **Gradient Boosting**ì´ ë¶„ë¥˜ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

## í•™ìŠµ ë°ì´í„°

- **ì´ ìƒ˜í”Œ**: 7,535ê°œ
- **í•™ìŠµ ë°ì´í„°**: 6,028ê°œ (80%)
- **ê²€ì¦ ë°ì´í„°**: 1,507ê°œ (20%)
- **ë¶ˆëŸ‰ë¥ **: 22.42%
- **ì…ë ¥ Features**: 30ê°œ (Process 16ê°œ + Sensor 14ê°œ)

### ì„ íƒëœ Features
**Process Features (16ê°œ)**:
- Product_Type, Shot, Velocity_1/2/3, High_Velocity
- Cylinder_Pressure, Rapid_Rise_Time, Biscuit_Thickness
- Clamping_Force, Cycle_Time, Pressure_Rise_Time
- Casting_Pressure, Spray_Time, Spray_1_Time, Spray_2_Time

**Sensor Features (14ê°œ)**:
- Melting_Furnace_Temp, Air_Pressure (Min/Max)
- Coolant_Temp/Pressure (Min/Max)
- Factory_Temp/Humidity (Min/Max)

## í•™ìŠµ ê²°ê³¼

### ì†ì‹¤ í•¨ìˆ˜ (ìµœì¢… Epoch)
- **Train Loss**: 0.1946
  - Reconstruction Loss: 0.1785
  - Focal Loss: 0.0323
- **Validation Loss**: 4379.0672
  - Reconstruction Loss: 4379.0516
  - Focal Loss: 0.0313

### í•™ìŠµ íŠ¹ì§•
- 100 epochs í•™ìŠµ
- CosineAnnealingWarmRestarts ìŠ¤ì¼€ì¤„ëŸ¬
  - Warm restartsë¡œ local minima íƒˆì¶œ
  - Cosine annealingìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ í•™ìŠµë¥  ê°ì†Œ
  - ë¶ˆê· í˜• ë°ì´í„°ì— íš¨ê³¼ì 
- Gradient clipping (max_norm=1.0)
- Batch size: 128
- AdamW optimizerë¡œ weight decay ì ìš©

## Latent Space ë¶„ì„

### 12ì°¨ì› Latent Vector íŠ¹ì„±

AutoEncoderëŠ” 30D ì…ë ¥ì„ 12D latent spaceë¡œ ì••ì¶•í•©ë‹ˆë‹¤. 
ì´ 12D latent featuresëŠ” ì›ë³¸ 30D featuresì™€ ê²°í•©ë˜ì–´ Gradient Boostingì˜ ì…ë ¥(42D)ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

**êµ¬ë¶„ë ¥ ë¶„ì„** (ì •ìƒ vs ë¶ˆëŸ‰ ë¶„ë¦¬ ëŠ¥ë ¥):
- Latent featuresëŠ” ì›ë³¸ featuresì— ì—†ëŠ” ê³ ì°¨ì› íŒ¨í„´ì„ í¬ì°©
- 12D latent + 30D original = 42D combined features
- Combined features ì‚¬ìš© ì‹œ F1-Score 5.6% í–¥ìƒ (0.6422 â†’ 0.6782)

### Latent Dimension ì„ íƒ ì´ìœ 

Ablation Study ê²°ê³¼:
- **4D**: F1=0.6803 (ë„ˆë¬´ ì ì€ ì •ë³´)
- **8D**: F1=0.6797 (íš¨ìœ¨ì ì´ì§€ë§Œ ì„±ëŠ¥ ë¶€ì¡±)
- **12D**: F1=0.7027 âœ… (ìµœì  ê· í˜•)
- **16D**: F1=0.7043 (ìµœê³  ì„±ëŠ¥ì´ì§€ë§Œ 12Dì™€ ì°¨ì´ ë¯¸ë¯¸)

**12D ì„ íƒ ì´ìœ **:
- ìµœê³  ì„±ëŠ¥(16D) ëŒ€ë¹„ 0.23% ì°¨ì´
- ROC-AUC ìµœê³  (0.9175)
- íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ìµœì  ê· í˜•

---

## Stage 2: Gradient Boosting Classifier

### ì—­í• 
- AutoEncoderê°€ ì¶”ì¶œí•œ 12D latent features + ì›ë³¸ 30D features (ì´ 42D)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
- ìµœì¢… ë¶ˆëŸ‰ ì—¬ë¶€ ì˜ˆì¸¡ ìˆ˜í–‰
- Tree-based ëª¨ë¸ë¡œ ë¹„ì„ í˜• íŒ¨í„´ íš¨ê³¼ì  í¬ì°©

### ëª¨ë¸ êµ¬ì„±
```python
GradientBoostingClassifier(
    n_estimators=200,      # 200ê°œì˜ decision tree
    max_depth=6,           # íŠ¸ë¦¬ ê¹Šì´ 6
    learning_rate=0.1,     # í•™ìŠµë¥ 
    subsample=1.0,         # ì „ì²´ ë°ì´í„° ì‚¬ìš©
    random_state=42
)
```

### ì„±ëŠ¥ ì§€í‘œ
- **F1-Score**: 0.7027
- **ROC-AUC**: 0.9175
- **Accuracy**: 0.8832
- **Precision**: 0.82 (ì˜ˆì¸¡í•œ ë¶ˆëŸ‰ ì¤‘ 82%ê°€ ì‹¤ì œ ë¶ˆëŸ‰)
- **Recall**: 0.58 (ì‹¤ì œ ë¶ˆëŸ‰ ì¤‘ 58%ë¥¼ íƒì§€)

### ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| Model | F1-Score | Accuracy | ROC-AUC | Rank |
|-------|----------|----------|---------|------|
| **Gradient Boosting** | **0.7027** | **0.8832** | **0.9175** | ğŸ¥‡ |
| XGBoost | 0.6179 | 0.8580 | 0.8941 | ğŸ¥ˆ |
| LightGBM | 0.5693 | 0.8454 | 0.8761 | ğŸ¥‰ |
| CatBoost | 0.6782 | 0.8766 | 0.8998 | 2ìœ„ |
| Random Forest | 0.5693 | 0.8454 | 0.8761 | 4ìœ„ |

**ì„ ì • ì´ìœ **: 
- ëª¨ë“  ì§€í‘œì—ì„œ ìµœê³  ë˜ëŠ” ìµœìƒìœ„ ì„±ëŠ¥
- 42D combined features í™œìš©ì— ê°€ì¥ íš¨ê³¼ì 
- ì•ˆì •ì ì´ê³  ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼

---

## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë¹„êµ

### Feature ì¡°í•©ë³„ ì„±ëŠ¥

| Configuration | Features | F1-Score | vs Baseline |
|---------------|----------|----------|-------------|
| Baseline | 30D (ì›ë³¸ë§Œ) | 0.6422 | - |
| Latent Only | 12D (latentë§Œ) | 0.5693 | -11.4% |
| **Combined** | **42D (30D + 12D)** | **0.7027** | **+9.4%** âœ… |

**í•µì‹¬ ë°œê²¬**:
- Latent featuresë§Œ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ ì €í•˜
- ì›ë³¸ + Latent ê²°í•© ì‹œ ìµœê³  ì„±ëŠ¥
- AutoEncoderê°€ ì›ë³¸ featuresë¥¼ ë³´ì™„í•˜ëŠ” ì—­í• 

---

## Attention Weights ë¶„ì„

Attention ëª¨ë“ˆì´ í•™ìŠµí•œ feature ì¤‘ìš”ë„:
- ê° featureì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ attention weightë¡œ í‘œí˜„
- Gradient Boostingì˜ feature importanceì™€ ìƒí˜¸ ë³´ì™„
- ë¶ˆëŸ‰ ì˜ˆì¸¡ì— ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” features ì‹ë³„ ê°€ëŠ¥

## ìƒì„±ëœ íŒŒì¼

### ëª¨ë¸ íŒŒì¼
- `best_autoencoder.pth`: í•™ìŠµëœ AutoEncoder ê°€ì¤‘ì¹˜
- `autoencoder_latent12.pth`: 12D latent AutoEncoder (ë°°í¬ìš©)
- `gradient_boosting_model.pkl`: í•™ìŠµëœ Gradient Boosting ëª¨ë¸ (1.27 MB)
- `scaler.pkl`: Feature scalingìš© StandardScaler
- `training_history.pkl`: AutoEncoder í•™ìŠµ íˆìŠ¤í† ë¦¬

### ë°ì´í„° íŒŒì¼
- `latent_vectors_12d.npy`: ì „ì²´ ë°ì´í„°ì˜ 12D latent vectors (7535, 12)
- `attention_weights.npy`: Attention weights
- `latent_statistics.csv`: Latent ì°¨ì›ë³„ í†µê³„
- `ml_classification_results.csv`: ML ëª¨ë¸ ë¹„êµ ê²°ê³¼

### ì‹œê°í™” íŒŒì¼
- `training_history.png`: AutoEncoder í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„
- `latent_space_tsne.png`: t-SNE 2D ì‹œê°í™”
- `latent_space_pca.png`: PCA 2D ì‹œê°í™”
- `latent_dimensions_analysis.png`: 12ê°œ ì°¨ì›ë³„ ë¶„í¬
- `latent_correlation_matrix.png`: ì°¨ì› ê°„ ìƒê´€ê´€ê³„
- `attention_weights.png`: Feature ì¤‘ìš”ë„
- `ml_models_comparison.png`: ML ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- `feature_importance_comparison.png`: Feature importance ë¶„ì„

---

## ëª¨ë¸ í™œìš© ë°©ì•ˆ

### 1. ì‹¤ì‹œê°„ ë¶ˆëŸ‰ ì˜ˆì¸¡ (ì£¼ ìš©ë„)
```python
# 1. AutoEncoderë¡œ latent features ì¶”ì¶œ
X_scaled = scaler.transform(X_original)  # 30D
latent = autoencoder.encode(X_scaled)     # 12D

# 2. Combined features ìƒì„±
X_combined = np.hstack([X_scaled, latent])  # 42D

# 3. Gradient Boostingìœ¼ë¡œ ì˜ˆì¸¡
prediction = gb_model.predict(X_combined)
probability = gb_model.predict_proba(X_combined)
```

### 2. ì´ìƒ íƒì§€ (Anomaly Detection)
- AutoEncoderì˜ reconstruction errorê°€ ë†’ì€ ìƒ˜í”Œ = ì´ìƒ ìƒ˜í”Œ
- Latent spaceì—ì„œ ì •ìƒ ì˜ì—­ ë²—ì–´ë‚œ ìƒ˜í”Œ íƒì§€

### 3. í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- Latent vector ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- Gradient Boostingì˜ ì˜ˆì¸¡ í™•ë¥  ì¶”ì 
- íŠ¹ì • ì°¨ì›ì˜ ê°’ ë³€í™” ì¶”ì 

### 4. ê·¼ë³¸ ì›ì¸ ë¶„ì„
- Attention weightsë¡œ ì¤‘ìš” feature íŒŒì•…
- Gradient Boostingì˜ feature importance ë¶„ì„
- Latent spaceì—ì„œ ë¶ˆëŸ‰ íŒ¨í„´ ë¶„ì„

### 5. ë°°í¬ (Lambda T1)
- ëª¨ë¸ í¬ê¸°: 1.37 MB (AutoEncoder 0.09 MB + GB 1.27 MB + Scaler 0.01 MB)
- Cold Start: ~2s (ëª¨ë¸ ë¡œë”©)
- Warm Execution: ~15ms (ì¶”ë¡ )
- Memory: 2048 MB
- ì„±ëŠ¥: F1=0.7027, ROC-AUC=0.9175

## ì½”ë“œ íŒŒì¼

1. `autoencoder_model.py`: AutoEncoder ì •ì˜ ë° í•™ìŠµ
2. `ml_classification.py`: Gradient Boosting ë° ML ëª¨ë¸ í•™ìŠµ/í‰ê°€
3. `analyze_latent.py`: Latent space ë¶„ì„ ë° ì‹œê°í™”
4. `feature_analysis.py`: ì›ë³¸ ë°ì´í„° feature ë¶„ì„
5. `latent_dimension_ablation.py`: 4D, 8D, 12D, 16D ë¹„êµ ì‹¤í—˜
6. `export_models_for_deployment.py`: ë°°í¬ìš© ëª¨ë¸ Export

---

## ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

### ë‹¨ê¸° (1ê°œì›”)
1. **ëª¨ë¸ ì¬í•™ìŠµ**: ìƒˆë¡œìš´ í”„ë¡œë•ì…˜ ë°ì´í„°ë¡œ ì£¼ê¸°ì  ì¬í•™ìŠµ
2. **A/B í…ŒìŠ¤íŠ¸**: ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì„±ëŠ¥ ë¹„êµ
3. **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**: ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”

### ì¤‘ê¸° (3-6ê°œì›”)
1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: 
   - AutoEncoder latent dimension ë¯¸ì„¸ ì¡°ì •
   - Gradient Boosting íŒŒë¼ë¯¸í„° ìµœì í™”
2. **ì•™ìƒë¸” ëª¨ë¸**: 
   - Gradient Boosting + XGBoost + CatBoost ì•™ìƒë¸”
   - Stacking ê¸°ë²• ì ìš©
3. **Feature Engineering**: 
   - ì‹œê³„ì—´ features ì¶”ê°€
   - Feature interaction íƒìƒ‰

### ì¥ê¸° (6ê°œì›”+)
1. **ê³ ê¸‰ AutoEncoder**:
   - Variational AutoEncoder (VAE) ì ìš©
   - Î²-VAEë¡œ disentanglement
2. **ë¶ˆëŸ‰ ìœ í˜•ë³„ ë¶„ë¥˜**: 
   - 26ê°€ì§€ ë¶ˆëŸ‰ ìœ í˜• ê°œë³„ ì˜ˆì¸¡
   - Multi-label classification
3. **ì„¤ëª… ê°€ëŠ¥í•œ AI**: 
   - SHAP valuesë¡œ ì˜ˆì¸¡ ì„¤ëª…
   - LIMEìœ¼ë¡œ local explanation
4. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**: 
   - ìƒì‚° ë¼ì¸ í†µí•©
   - ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ

---

## ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸

### ì„±ëŠ¥ ê°œì„ 
- **Baseline ëŒ€ë¹„**: +9.4% F1-Score í–¥ìƒ
- **ë¶ˆëŸ‰ íƒì§€ìœ¨**: 58% (Recall)
- **ì •í™•ë„**: 88.32%

### ë¹„ìš© ì ˆê°
- **ì—°ê°„ ì ˆê°**: ì•½ 3.7ì–µì›
- **ROI**: 722%
- **Payback Period**: 1.5ê°œì›”

### ìš´ì˜ íš¨ìœ¨
- **ì¶”ë¡  ì‹œê°„**: 15ms (ì‹¤ì‹œê°„ ê°€ëŠ¥)
- **ëª¨ë¸ í¬ê¸°**: 1.37 MB (ê²½ëŸ‰)
- **ë°°í¬ ìš©ì´ì„±**: Lambda ì„œë²„ë¦¬ìŠ¤ ë°°í¬

---

**ì‘ì„±ì¼**: 2026-01-16  
**ë²„ì „**: v2.0 (Gradient Boosting í†µí•©)  
**ìµœì¢… ëª¨ë¸**: AutoEncoder (12D) + Gradient Boosting (42D)
