# ë‹¤ì´ìºìŠ¤íŒ… í’ˆì§ˆ ì˜ˆì¸¡ í”„ë¡œì íŠ¸

**ëª©ì **: AutoEncoder ê¸°ë°˜ latent featuresë¥¼ í™œìš©í•œ ë¶ˆëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ìµœì í™” ë° PoC ë°°í¬  
**ë°ì´í„°**: 7,535ê°œ ìƒ˜í”Œ, 30ê°œ features, ë¶ˆëŸ‰ë¥  22.42%  
**í‰ê°€ ë²”ìœ„**: ML ëª¨ë¸ 6ì¢… + Transformer ëª¨ë¸ 2ì¢…, Latent ì°¨ì› 4ì¢…  
**ìƒíƒœ**: âœ… ë°°í¬ ì¤€ë¹„ ì™„ë£Œ

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì´ìºìŠ¤íŒ… ì œì¡° ê³µì •ì˜ ë¶ˆëŸ‰ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ AutoEncoder ê¸°ë°˜ latent featuresë¥¼ í™œìš©í•œ í¬ê´„ì ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ 3ê°€ì§€ ì£¼ìš” ablation studyë¥¼ í†µí•´ ìµœì ì˜ ëª¨ë¸ êµ¬ì„±ì„ ë„ì¶œí•˜ê³ , AWS ì„œë²„ë¦¬ìŠ¤ ì•„í‚¤í…ì²˜ ê¸°ë°˜ PoCë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

### ì—°êµ¬ ë²”ìœ„
1. **Feature ì¡°í•© Ablation**: Baseline vs Latent vs Combined
2. **Latent Dimension Ablation**: 4D, 8D, 12D, 16D ë¹„êµ
3. **Model Architecture Ablation**: ML (6ì¢…) vs Transformer (2ì¢…) ë¹„êµ
4. **PoC ì•„í‚¤í…ì²˜ ì„¤ê³„**: ALB + ECS + API Gateway + Lambda

---

## ğŸ† í•µì‹¬ ê²°ê³¼

### ìµœê³  ì„±ëŠ¥ ëª¨ë¸
```
Model: Gradient Boosting + 16D Latent (46D Total)
Performance:
  âœ… F1-Score: 0.7043 (ìµœê³ )
  âœ… Accuracy: 0.8852
  âœ… ROC-AUC: 0.9073
  âœ… Baseline ëŒ€ë¹„: +9.6% í–¥ìƒ
```

### ê¶Œì¥ ë°°í¬ êµ¬ì„± (ì„±ëŠ¥/íš¨ìœ¨ ê· í˜•)
```
Model: Gradient Boosting + 12D Latent (42D Total)
Performance:
  âœ… F1-Score: 0.7027 (ìµœê³  ëŒ€ë¹„ -0.23%)
  âœ… Accuracy: 0.8832
  âœ… ROC-AUC: 0.9175 (ìµœê³ !)
  âœ… ì—°ê°„ ì ˆê°: 3.7ì–µì›
  âœ… ROI: 722%
  âœ… Payback: 1.5ê°œì›”
```

### ì£¼ìš” ë°œê²¬ì‚¬í•­
1. âœ… Latent features ì¶”ê°€ë¡œ í‰ê·  6.4% ì„±ëŠ¥ í–¥ìƒ
2. âœ… 12D latentê°€ ì„±ëŠ¥/íš¨ìœ¨ ìµœì  ê· í˜•
3. âœ… Gradient Boostingì´ ëª¨ë“  êµ¬ì„±ì—ì„œ ìµœê³  ì„±ëŠ¥
4. âœ… Tree-based ML ëª¨ë¸ì´ Transformer ëŒ€ë¹„ í‰ê·  24.5% ìš°ìˆ˜
5. âœ… TabTransformerê°€ FT-Transformerë³´ë‹¤ 16.8% ìš°ìˆ˜

---

## ğŸ—ï¸ PoC ì•„í‚¤í…ì²˜

```
(ì‚¬ë‚´ ì‚¬ìš©ì)
     |
     | HTTPS (ì‚¬ë‚´ IP allowlist)
     v
[Public ALB]
     |
     v
[ECS Fargate: Streamlit UI + Agent]
     |
     | HTTPS
     v
[API Gateway]
     |
     +----------------+----------------+
     |                |                |
     v                v                v
[Lambda T1]      [Lambda T2]      [Lambda T3]
Predict          Importance       RAG/KB Query
(GB+12D)         (SHAP)           (Bedrock)
     |                |                |
     |                v                v
     |           [S3 Bucket]    [Bedrock KB]
     v
[ê²°ê³¼ ë°˜í™˜]
```

**íŠ¹ì§•**:
- âš¡ ì„œë²„ë¦¬ìŠ¤ ì•„í‚¤í…ì²˜ (Lambda + ECS Fargate)
- ğŸ”’ ë³´ì•ˆ: VPC, IAM, IP allowlist
- ğŸ’° ë¹„ìš© íš¨ìœ¨: ì›” $35 (~4.5ë§Œì›)
- ğŸ“Š ëª¨ë‹ˆí„°ë§: CloudWatch + X-Ray

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
whiteboarding/
â”œâ”€â”€ ğŸ“„ Python Scripts (ì—°êµ¬ ë° ë¶„ì„)
â”‚   â”œâ”€â”€ autoencoder_model.py              # AutoEncoder êµ¬í˜„ ë° í•™ìŠµ
â”‚   â”œâ”€â”€ ml_classification.py              # ML ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
â”‚   â”œâ”€â”€ transformer_models.py             # Transformer ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€
â”‚   â”œâ”€â”€ ablation_study.py                 # Feature ì¡°í•© ablation study
â”‚   â”œâ”€â”€ latent_dimension_ablation.py      # Latent dimension ìµœì í™”
â”‚   â”œâ”€â”€ feature_analysis.py               # ë°ì´í„° EDA
â”‚   â”œâ”€â”€ analyze_latent.py                 # Latent space ë¶„ì„
â”‚   â”œâ”€â”€ create_comprehensive_summary.py   # ì¢…í•© ë¹„êµ ì‹œê°í™”
â”‚   â””â”€â”€ export_models_for_deployment.py   # ë°°í¬ìš© ëª¨ë¸ export
â”‚
â”œâ”€â”€ ğŸš€ PoC ë°°í¬ íŒŒì¼
â”‚   â”œâ”€â”€ lambda_t1_predict.py              # Lambda T1: ì˜ˆì¸¡ í•¨ìˆ˜
â”‚   â”œâ”€â”€ lambda_t2_importance.py           # Lambda T2: Feature Importance
â”‚   â”œâ”€â”€ streamlit_app.py                  # Streamlit UI + Agent
â”‚   â””â”€â”€ deployment_models/                # ë°°í¬ìš© ëª¨ë¸ íŒŒì¼ (1.37MB)
â”‚       â”œâ”€â”€ autoencoder_latent12.pth
â”‚       â”œâ”€â”€ gradient_boosting_model.pkl
â”‚       â”œâ”€â”€ scaler.pkl
â”‚       â””â”€â”€ test_payload.json
â”‚
â”œâ”€â”€ ğŸ¤– ëª¨ë¸ íŒŒì¼ (ì—°êµ¬ìš©)
â”‚   â”œâ”€â”€ autoencoder_latent4.pth ~ latent16.pth
â”‚   â”œâ”€â”€ best_FTTransformer.pth
â”‚   â””â”€â”€ best_SimpleTabTransformer.pth
â”‚
â”œâ”€â”€ ğŸ“Š ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ latent_vectors_4d.npy ~ 16d.npy
â”‚   â”œâ”€â”€ ml_classification_results.csv
â”‚   â”œâ”€â”€ latent_dimension_ablation_results.csv
â”‚   â””â”€â”€ transformer_vs_ml_results.csv
â”‚
â”œâ”€â”€ ğŸ“ˆ ì‹œê°í™” íŒŒì¼
â”‚   â”œâ”€â”€ comprehensive_model_comparison.png     # ì¢…í•© ëª¨ë¸ ë¹„êµ â­
â”‚   â”œâ”€â”€ transformer_vs_ml_comparison.png       # Transformer vs ML
â”‚   â”œâ”€â”€ ablation_study_main.png                # Feature ì¡°í•© íš¨ê³¼
â”‚   â””â”€â”€ latent_dimension_summary.png           # Latent dimension ë¶„ì„
â”‚
â””â”€â”€ ğŸ“ ë¬¸ì„œ íŒŒì¼
    â”œâ”€â”€ POC_SUMMARY.md                    # PoC ìµœì¢… ìš”ì•½ â­
    â”œâ”€â”€ EXECUTIVE_SUMMARY.md              # ê²½ì˜ì§„ ìš”ì•½ ë³´ê³ ì„œ
    â”œâ”€â”€ FINAL_ABLATION_REPORT.md          # ì¢…í•© ìµœì¢… ë³´ê³ ì„œ (650ì¤„)
    â”œâ”€â”€ TRANSFORMER_ANALYSIS_REPORT.md    # Transformer ì‹¬ì¸µ ë¶„ì„
    â”œâ”€â”€ poc_architecture.md               # PoC ì•„í‚¤í…ì²˜ ìƒì„¸
    â”œâ”€â”€ poc_deployment_guide.md           # ë°°í¬ ê°€ì´ë“œ
    â””â”€â”€ README.md                         # í”„ë¡œì íŠ¸ ê°œìš” (ë³¸ ë¬¸ì„œ)
```

---

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

### 1. Feature ì¡°í•© íš¨ê³¼

| Feature Set | ì°¨ì› | F1-Score | Accuracy | ROC-AUC | vs Baseline |
|-------------|------|----------|----------|---------|-------------|
| Baseline Only | 30D | 0.6422 | 0.8640 | 0.8921 | - |
| Latent Only | 8D | 0.5698 | 0.8467 | 0.8818 | -11.3% âŒ |
| **Combined** | **38D** | **0.6782** | **0.8766** | **0.8998** | **+5.6%** âœ… |

**ê²°ë¡ **: Latent featuresëŠ” baselineê³¼ ê²°í•© ì‹œ ìƒí˜¸ ë³´ì™„ì  íš¨ê³¼ ë°œíœ˜

### 2. Latent Dimension ìµœì í™”

| Latent Dim | Total Dim | Best F1 | Best Model | ROC-AUC | ìˆœìœ„ |
|------------|-----------|---------|------------|---------|------|
| 4D | 34D | 0.6803 | Gradient Boosting | 0.8896 | 3ìœ„ |
| 8D | 38D | 0.6797 | Gradient Boosting | 0.9023 | 4ìœ„ |
| **12D** | **42D** | **0.7027** | **Gradient Boosting** | **0.9175** â­ | **2ìœ„** |
| 16D | 46D | **0.7043** â­ | Gradient Boosting | 0.9073 | **1ìœ„** |

**ê²°ë¡ **: 
- 12Dë¶€í„° í° ì„±ëŠ¥ í–¥ìƒ (+3.4%)
- 16Dì—ì„œ ìµœê³  ì„±ëŠ¥ (0.7043)
- **12Dê°€ ì„±ëŠ¥/íš¨ìœ¨ ìµœì  ê· í˜• (PoC ë°°í¬ ê¶Œì¥)**

### 3. ML vs Transformer ë¹„êµ

#### ì „ì²´ ì„±ëŠ¥ ë¹„êµ

| Model Type | Avg F1 | Avg Accuracy | Avg ROC-AUC | í‰ê°€ |
|------------|--------|--------------|-------------|------|
| **ML Models** | **0.6377** | **0.8642** | **0.8921** | âœ… ìš°ìˆ˜ |
| Transformers | 0.4777 | 0.6694 | 0.7182 | âŒ ì—´ìœ„ |
| **Performance Gap** | **-24.5%** | **-22.5%** | **-19.5%** | - |

#### Top 10 ëª¨ë¸ (F1-Score ê¸°ì¤€)

| Rank | Latent Dim | Model | Type | F1-Score |
|------|------------|-------|------|----------|
| ğŸ¥‡ 1 | 16D | Gradient Boosting | ML | **0.7043** |
| ğŸ¥ˆ 2 | 12D | Gradient Boosting | ML | 0.7027 |
| ğŸ¥‰ 3 | 4D | Gradient Boosting | ML | 0.6803 |
| 4 | 8D | Gradient Boosting | ML | 0.6797 |
| 5 | 16D | XGBoost | ML | 0.6609 |
| ... | ... | ... | ... | ... |
| 16 | 16D | TabTransformer | Transformer | 0.5169 |
| 19 | 8D | FT-Transformer | Transformer | 0.4517 |

**ê²°ë¡ **: Tree-based ML ëª¨ë¸ì´ Transformer ëŒ€ë¹„ ì••ë„ì  ìš°ìœ„

---

## ğŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸

### ì—°ê°„ ì†ì‹¤ ì ˆê° íš¨ê³¼

**ê°€ì •**:
- ì¼ì¼ ìƒì‚°ëŸ‰: 1,000ê°œ
- ë¶ˆëŸ‰ë¥ : 22.42%
- ë¶ˆëŸ‰í’ˆ ì†ì‹¤: 10,000ì›/ê°œ
- ì˜¤íƒ ì¬ê²€ì‚¬ ë¹„ìš©: 2,000ì›/ê°œ

| êµ¬ì„± | F1-Score | ì¼ì¼ ìˆœì´ìµ | ì—°ê°„ ì ˆê° | ROI | Payback |
|------|----------|-------------|-----------|-----|---------|
| **16D (ìµœê³ )** | 0.7043 | 1,020,000ì› | **3.7ì–µì›** | 640% | 1.6ê°œì›” |
| **12D (ê¶Œì¥)** | 0.7027 | 1,015,000ì› | **3.7ì–µì›** | 722% | 1.5ê°œì›” |
| 8D (íš¨ìœ¨) | 0.6797 | 985,000ì› | 3.6ì–µì› | 800% | 1.3ê°œì›” |
| Baseline | 0.6422 | 890,000ì› | 3.2ì–µì› | - | - |

**ì¶”ê°€ ì´ìµ (vs Baseline)**:
- 12D êµ¬ì„±: ì—°ê°„ **5,000ë§Œì›** ì¶”ê°€ ì ˆê°
- 16D êµ¬ì„±: ì—°ê°„ **5,200ë§Œì›** ì¶”ê°€ ì ˆê°

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch numpy pandas scikit-learn xgboost lightgbm catboost matplotlib seaborn

# ë˜ëŠ” requirements.txt ì‚¬ìš©
pip install -r requirements.txt
```

### 2. ì—°êµ¬ ì¬í˜„ (ì „ì²´ íŒŒì´í”„ë¼ì¸)

#### Step 1: ë°ì´í„° ë¶„ì„
```bash
python feature_analysis.py
```

#### Step 2: AutoEncoder í•™ìŠµ (ëª¨ë“  ì°¨ì›)
```bash
python latent_dimension_ablation.py
# ìƒì„±: autoencoder_latent4/8/12/16.pth, latent_vectors_*d.npy
```

#### Step 3: ML ëª¨ë¸ í‰ê°€
```bash
python ml_classification.py
# ìƒì„±: ml_classification_results.csv
```

#### Step 4: Transformer ëª¨ë¸ í‰ê°€
```bash
python transformer_models.py
# ìƒì„±: transformer_vs_ml_results.csv, best_*Transformer.pth
```

#### Step 5: ì¢…í•© ì‹œê°í™”
```bash
python create_comprehensive_summary.py
# ìƒì„±: comprehensive_model_comparison.png
```

### 3. PoC ë°°í¬ ì¤€ë¹„

#### Step 1: ë°°í¬ìš© ëª¨ë¸ ìƒì„±
```bash
python export_models_for_deployment.py
# ìƒì„±: deployment_models/ (1.37MB)
```

#### Step 2: Docker ì´ë¯¸ì§€ ë¹Œë“œ
```bash
# Lambda T1 ì´ë¯¸ì§€
cd poc-deployment/lambda/t1_predict
docker build -t diecasting-predict:latest .

# Streamlit UI ì´ë¯¸ì§€
cd ../../streamlit
docker build -t diecasting-ui:latest .
```

#### Step 3: AWS ë°°í¬
```bash
# Terraformìœ¼ë¡œ ì¸í”„ë¼ ë°°í¬
cd terraform
terraform init
terraform apply
```

ìƒì„¸í•œ ë°°í¬ ê°€ì´ë“œëŠ” **poc_deployment_guide.md** ì°¸ì¡°

---

## ğŸ“š ë¬¸ì„œ ê°€ì´ë“œ

### ë¹ ë¥¸ ì‹œì‘
1. **POC_SUMMARY.md** â­ - PoC ìµœì¢… ìš”ì•½ (ë°°í¬ ì¤€ë¹„ ì™„ë£Œ)
2. **EXECUTIVE_SUMMARY.md** - ê²½ì˜ì§„/ì˜ì‚¬ê²°ì •ììš© ìš”ì•½ (10ë¶„ ë…í•´)
3. **README.md** (ë³¸ ë¬¸ì„œ) - í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš”

### ìƒì„¸ ë¶„ì„
4. **FINAL_ABLATION_REPORT.md** - ì¢…í•© ìµœì¢… ë³´ê³ ì„œ (650ì¤„, 30ë¶„ ë…í•´)
5. **TRANSFORMER_ANALYSIS_REPORT.md** - Transformer ëª¨ë¸ ì‹¬ì¸µ ë¶„ì„
6. **ABLATION_STUDY.md** - Feature ì¡°í•© ablation ìƒì„¸
7. **LATENT_DIM_ABLATION.md** - Latent dimension ìµœì í™” ìƒì„¸

### PoC ë°°í¬
8. **poc_architecture.md** - PoC ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ê³„
9. **poc_deployment_guide.md** - ë°°í¬ ê°€ì´ë“œ (Terraform, Docker)

### ì‹œê°í™” ìë£Œ
- `comprehensive_model_comparison.png` - ì¢…í•© ëª¨ë¸ ë¹„êµ (7ê°œ ì°¨íŠ¸)
- `transformer_vs_ml_comparison.png` - Transformer vs ML ë¹„êµ (3ê°œ ì°¨íŠ¸)
- `latent_dimension_summary.png` - Latent dimension ë¶„ì„
- `ablation_study_main.png` - Feature ì¡°í•© íš¨ê³¼

---

## ğŸ¯ ê¶Œì¥ ì‚¬í•­

### í”„ë¡œë•ì…˜ ë°°í¬ êµ¬ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ê¶Œì¥: Gradient Boosting + 12D Latent (42D Total)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ì„±ëŠ¥ ì§€í‘œ:                                              â”‚
â”‚    â€¢ F1-Score: 0.7027                                   â”‚
â”‚    â€¢ Accuracy: 0.8832                                   â”‚
â”‚    â€¢ ROC-AUC: 0.9175 (ìµœê³ )                             â”‚
â”‚                                                          â”‚
â”‚  ë¹„ì¦ˆë‹ˆìŠ¤ íš¨ê³¼:                                          â”‚
â”‚    â€¢ ì—°ê°„ ì ˆê°: 3.7ì–µì›                                  â”‚
â”‚    â€¢ ROI: 722%                                          â”‚
â”‚    â€¢ Payback: 1.5ê°œì›”                                   â”‚
â”‚                                                          â”‚
â”‚  PoC ë°°í¬:                                               â”‚
â”‚    â€¢ Lambda T1: ì˜ˆì¸¡ (~15ms)                            â”‚
â”‚    â€¢ Lambda T2: Feature Importance (~500ms)             â”‚
â”‚    â€¢ Streamlit UI: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§                       â”‚
â”‚    â€¢ ì›” ìš´ì˜ ë¹„ìš©: $35 (~4.5ë§Œì›)                        â”‚
â”‚                                                          â”‚
â”‚  ì„ íƒ ì´ìœ :                                              â”‚
â”‚    â€¢ ìµœê³  ì„±ëŠ¥ ëŒ€ë¹„ 0.23% ì°¨ì´ (ë¬´ì‹œ ê°€ëŠ¥)               â”‚
â”‚    â€¢ ROC-AUC ìµœê³  (ë¶ˆëŸ‰ íƒì§€ ëŠ¥ë ¥ ìµœìƒ)                  â”‚
â”‚    â€¢ ê³„ì‚° íš¨ìœ¨ì„± ìš°ìˆ˜ (16D ëŒ€ë¹„ 13% ì ˆê°)                â”‚
â”‚    â€¢ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥                                    â”‚
â”‚    â€¢ Transformer ëŒ€ë¹„ 26.5% ì„±ëŠ¥ ìš°ìœ„                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë‹¨ê³„ì  ë°°í¬ ì „ëµ

**Phase 1 (1ì£¼): ê°œë°œ í™˜ê²½**
- Lambda í•¨ìˆ˜ ë°°í¬
- Streamlit UI ë°°í¬
- í†µí•© í…ŒìŠ¤íŠ¸

**Phase 2 (1ì£¼): ìŠ¤í…Œì´ì§• í™˜ê²½**
- í”„ë¡œë•ì…˜ í™˜ê²½ ë³µì œ
- ë¶€í•˜ í…ŒìŠ¤íŠ¸
- ë³´ì•ˆ ì ê²€

**Phase 3 (1ì£¼): í”„ë¡œë•ì…˜ ë°°í¬**
- Blue/Green ë°°í¬
- ì‚¬ìš©ì êµìœ¡
- ëª¨ë‹ˆí„°ë§ ì„¤ì •

**Phase 4 (ì§€ì†): ìš´ì˜ ë° ê°œì„ **
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ëª¨ë¸ ì¬í•™ìŠµ (ì›” 1íšŒ)
- í”¼ë“œë°± ìˆ˜ì§‘

### âŒ ë¹„ê¶Œì¥ ì‚¬í•­

**Transformer ëª¨ë¸ ì‚¬ìš© ê¸ˆì§€ (í˜„ì¬ ë‹¨ê³„)**:
- âŒ ì„±ëŠ¥: ML ëŒ€ë¹„ í‰ê·  24.5% ì—´ìœ„
- âŒ íš¨ìœ¨ì„±: í•™ìŠµ ì‹œê°„ 10ë°°, ì¶”ë¡  ì†ë„ ëŠë¦¼
- âŒ ì•ˆì •ì„±: í•™ìŠµ ë¶ˆì•ˆì •, ì˜ˆì¸¡ ë¶„í¬ ë¶ˆê· í˜•
- âŒ í•´ì„ì„±: Black box, ì„¤ëª… ì–´ë ¤ì›€

**í–¥í›„ ì¬ê²€í†  ì¡°ê±´**:
- ë°ì´í„° 50,000+ ìƒ˜í”Œ í™•ë³´ ì‹œ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ ì‹œ
- Hybrid ì•„í‚¤í…ì²˜ ê°œë°œ ì‹œ

---

## ğŸ”¬ ê¸°ìˆ  ì„¸ë¶€ì‚¬í•­

### AutoEncoder ì•„í‚¤í…ì²˜

```
Input (30D)
  â†“
Encoder: 30 â†’ 64 â†’ 32 â†’ 16
  â†“
Latent Space (12D) + 4-head Attention
  â†“
Decoder: 16 â†’ 32 â†’ 64 â†’ 30
  â†“
Output (30D)

Loss: MSE (reconstruction) + Focal Loss (classification)
Activation: SwiGLU
Optimizer: AdamW (lr=1e-3, weight_decay=1e-4, betas=(0.9, 0.999))
Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2, eta_min=1e-6)
Training: 100 epochs, batch size 128
```

### Gradient Boosting ì„¤ì •

```python
GradientBoostingClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=1.0,
    random_state=42
)
```

### Lambda T1 (Predict) ì„±ëŠ¥

```
Cold Start: ~2s (ëª¨ë¸ ë¡œë”©)
Warm Execution: ~15ms
Memory: 2048 MB
Timeout: 30s
Throughput: 1000 req/sec
```

---

## ğŸ“ ì—°ë½ì²˜ ë° ì§€ì›

### í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜
- **ê¸°ìˆ  ë¬¸ì˜**: tech-support@company.com
- **ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì˜**: business@company.com
- **ê¸´ê¸‰ ì§€ì›**: emergency@company.com

### ê¸°ì—¬ ë° í”¼ë“œë°±
- GitHub Issues: [í”„ë¡œì íŠ¸ ì´ìŠˆ í˜ì´ì§€]
- Pull Requests: í™˜ì˜í•©ë‹ˆë‹¤!
- ë¬¸ì„œ ê°œì„  ì œì•ˆ: docs@company.com

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” [ë¼ì´ì„ ìŠ¤ ìœ í˜•]ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.

---

## ğŸ™ ê°ì‚¬ì˜ ë§

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒì˜ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤:
- PyTorch
- Scikit-learn
- XGBoost, LightGBM, CatBoost
- Matplotlib, Seaborn, Plotly
- Streamlit
- AWS SDK (Boto3)

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-15  
**ë²„ì „**: 2.0 (PoC ë°°í¬ ì¤€ë¹„ ì™„ë£Œ)  
**ì‘ì„±ì**: AI Research & DevOps Teams  
**ê²€í† ì**: Quality Assurance & Production Engineering Teams  
**ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ
