# Transformer Models vs ML Models ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ

**í”„ë¡œì íŠ¸**: ë‹¤ì´ìºìŠ¤íŒ… í’ˆì§ˆ ì˜ˆì¸¡ - Transformer ì•„í‚¤í…ì²˜ í‰ê°€  
**ì‘ì„±ì¼**: 2026-01-15  
**ëª©ì **: FT-Transformer ë° TabTransformerì˜ ì„±ëŠ¥ì„ ê¸°ì¡´ ML ëª¨ë¸ê³¼ ë¹„êµ ë¶„ì„

---

## Executive Summary

ë³¸ ì—°êµ¬ëŠ” tabular dataì— íŠ¹í™”ëœ transformer ì•„í‚¤í…ì²˜(FT-Transformer, TabTransformer)ë¥¼ ë‹¤ì´ìºìŠ¤íŒ… ë¶ˆëŸ‰ ì˜ˆì¸¡ taskì— ì ìš©í•˜ê³ , ê¸°ì¡´ tree-based ML ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ì„ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤.

### í•µì‹¬ ê²°ê³¼
- **ìµœê³  ì„±ëŠ¥ ëª¨ë¸**: Gradient Boosting (16D latent) - F1: 0.7043
- **ìµœê³  Transformer**: TabTransformer (16D latent) - F1: 0.5169
- **ì„±ëŠ¥ ê²©ì°¨**: ML ëª¨ë¸ì´ transformer ëŒ€ë¹„ **26.6% ìš°ìˆ˜**
- **ê²°ë¡ **: í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œëŠ” tree-based ML ëª¨ë¸ì´ transformerë³´ë‹¤ íš¨ê³¼ì 

---

## 1. í‰ê°€ ëª¨ë¸ ì†Œê°œ

### 1.1 FT-Transformer (Feature Tokenizer + Transformer)

**ì¶œì²˜**: "Revisiting Deep Learning Models for Tabular Data" (NeurIPS 2021)

**ì•„í‚¤í…ì²˜ íŠ¹ì§•**:
- ê° featureë¥¼ ë…ë¦½ì ì¸ tokenìœ¼ë¡œ ë³€í™˜
- CLS tokenì„ í†µí•œ classification
- Multi-head self-attentionìœ¼ë¡œ feature ê°„ ê´€ê³„ í•™ìŠµ
- Transformer encoder blocks (2 layers)

**êµ¬í˜„ ì„¤ì •**:
```python
FTTransformer(
    n_features=38/42/46,  # 30 + latent_dim
    d_token=96,           # Token dimension
    n_blocks=2,           # Transformer blocks
    n_heads=4,            # Attention heads
    attention_dropout=0.2,
    ffn_dropout=0.1
)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: 224,257

### 1.2 TabTransformer (Simplified)

**ì¶œì²˜**: "TabTransformer: Tabular Data Modeling Using Contextual Embeddings" (2020)

**ì•„í‚¤í…ì²˜ íŠ¹ì§•**:
- Feature projection + positional embeddings
- Transformer encoder (2 layers)
- Global average pooling
- ì—°ì†í˜• featuresì— ìµœì í™”ëœ ë‹¨ìˆœí™” ë²„ì „

**êµ¬í˜„ ì„¤ì •**:
```python
SimpleTabTransformer(
    n_features=38/42/46,
    d_model=64,
    n_heads=4,
    n_layers=2,
    dropout=0.1
)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: 106,881 ~ 107,393

### 1.3 í•™ìŠµ ì„¤ì •

**ê³µí†µ ì„¤ì •**:
- Optimizer: AdamW (lr=3e-4, weight_decay=1e-5)
- Loss: Weighted BCE (pos_weight=3.5, ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)
- Batch size: 128
- Max epochs: 100
- Early stopping: patience=15
- Gradient clipping: max_norm=1.0

---

## 2. ì‹¤í—˜ ê²°ê³¼

### 2.1 ì „ì²´ ì„±ëŠ¥ ë¹„êµ (F1-Score ê¸°ì¤€)

#### Top 10 ëª¨ë¸

| Rank | Latent Dim | Model | Type | F1-Score | Accuracy | ROC-AUC |
|------|------------|-------|------|----------|----------|---------|
| ğŸ¥‡ 1 | 16D | Gradient Boosting | ML | **0.7043** | 0.8852 | 0.9073 |
| ğŸ¥ˆ 2 | 12D | Gradient Boosting | ML | **0.7027** | 0.8832 | **0.9175** |
| ğŸ¥‰ 3 | 4D | Gradient Boosting | ML | 0.6803 | 0.8752 | 0.8896 |
| 4 | 8D | Gradient Boosting | ML | 0.6797 | 0.8806 | 0.9023 |
| 5 | 16D | XGBoost | ML | 0.6609 | 0.8693 | 0.8934 |
| 6 | 8D | XGBoost | ML | 0.6254 | 0.8593 | 0.8864 |
| 7 | 12D | XGBoost | ML | 0.6197 | 0.8567 | 0.8958 |
| 8 | 4D | XGBoost | ML | 0.6115 | 0.8567 | 0.8848 |
| 9 | 8D | LightGBM | ML | 0.6093 | 0.8553 | 0.8825 |
| 10 | 12D | LightGBM | ML | 0.6050 | 0.8527 | 0.8908 |

**Transformer ëª¨ë¸ ìˆœìœ„**:
- **16ìœ„**: TabTransformer (16D) - F1: 0.5169
- **17ìœ„**: TabTransformer (12D) - F1: 0.5165
- **18ìœ„**: TabTransformer (8D) - F1: 0.5104
- **19ìœ„**: FT-Transformer (8D) - F1: 0.4517
- **20ìœ„**: FT-Transformer (16D) - F1: 0.4506
- **21ìœ„**: FT-Transformer (12D) - F1: 0.4199

### 2.2 Latent Dimensionë³„ ìƒì„¸ ê²°ê³¼

#### 8D Latent (38D Total Features)

| Model | Type | F1-Score | Accuracy | ROC-AUC | vs Best ML |
|-------|------|----------|----------|---------|------------|
| Gradient Boosting | ML | 0.6797 | 0.8806 | 0.9023 | - |
| XGBoost | ML | 0.6254 | 0.8593 | 0.8864 | -8.0% |
| LightGBM | ML | 0.6093 | 0.8553 | 0.8825 | -10.4% |
| **TabTransformer** | Transformer | **0.5104** | 0.7034 | 0.7574 | **-24.9%** |
| **FT-Transformer** | Transformer | **0.4517** | 0.6344 | 0.6954 | **-33.5%** |

#### 12D Latent (42D Total Features)

| Model | Type | F1-Score | Accuracy | ROC-AUC | vs Best ML |
|-------|------|----------|----------|---------|------------|
| Gradient Boosting | ML | 0.7027 | 0.8832 | **0.9175** | - |
| XGBoost | ML | 0.6197 | 0.8567 | 0.8958 | -11.8% |
| LightGBM | ML | 0.6050 | 0.8527 | 0.8908 | -13.9% |
| **TabTransformer** | Transformer | **0.5165** | 0.6795 | 0.7575 | **-26.5%** |
| **FT-Transformer** | Transformer | **0.4199** | 0.6204 | 0.6442 | **-40.3%** |

#### 16D Latent (46D Total Features)

| Model | Type | F1-Score | Accuracy | ROC-AUC | vs Best ML |
|-------|------|----------|----------|---------|------------|
| Gradient Boosting | ML | **0.7043** | 0.8852 | 0.9073 | - |
| XGBoost | ML | 0.6609 | 0.8693 | 0.8934 | -6.2% |
| LightGBM | ML | 0.5917 | 0.8507 | 0.8830 | -16.0% |
| **TabTransformer** | Transformer | **0.5169** | 0.6961 | 0.7585 | **-26.6%** |
| **FT-Transformer** | Transformer | **0.4506** | 0.6828 | 0.6960 | **-36.0%** |

### 2.3 ëª¨ë¸ íƒ€ì…ë³„ í‰ê·  ì„±ëŠ¥

| Model Type | Avg F1-Score | Avg Accuracy | Avg ROC-AUC | Std F1 |
|------------|--------------|--------------|-------------|--------|
| **ML Models** | **0.6372** | **0.8643** | **0.8918** | 0.0413 |
| **Transformers** | **0.4810** | **0.6694** | **0.7175** | 0.0398 |
| **Performance Gap** | **-24.5%** | **-22.5%** | **-19.5%** | - |

---

## 3. ì‹¬ì¸µ ë¶„ì„

### 3.1 Transformer ëª¨ë¸ì˜ í•œê³„

#### 3.1.1 ë°ì´í„°ì…‹ í¬ê¸° ë¬¸ì œ
- **í•™ìŠµ ìƒ˜í”Œ**: 6,028ê°œ (train set)
- **Transformer íŒŒë¼ë¯¸í„°**: 106K ~ 224K
- **ë¬¸ì œ**: TransformerëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ê°•ì  ë°œíœ˜
- **ê²°ê³¼**: ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ê³¼ì í•© ê²½í–¥

#### 3.1.2 Tabular Data íŠ¹ì„±
- **Tree-based ëª¨ë¸ ê°•ì **:
  - Feature ê°„ ë¹„ì„ í˜• ê´€ê³„ íš¨ê³¼ì  í¬ì°©
  - Missing values ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
  - Feature importance ì§ê´€ì 
  - ì ì€ ë°ì´í„°ë¡œë„ ì¢‹ì€ ì„±ëŠ¥

- **Transformer ì•½ì **:
  - Tabular dataì˜ êµ¬ì¡°ì  íŠ¹ì„± í™œìš© ì œí•œ
  - Inductive bias ë¶€ì¡±
  - ëŒ€ê·œëª¨ ë°ì´í„° í•„ìš”

#### 3.1.3 í•™ìŠµ ì•ˆì •ì„±
- **ML ëª¨ë¸**: ì•ˆì •ì ì´ê³  ë¹ ë¥¸ ìˆ˜ë ´
- **Transformer**: 
  - í•™ìŠµ ë¶ˆì•ˆì • (early stopping ë¹ˆë²ˆ)
  - ì˜ˆì¸¡ ë¶„í¬ ë¶ˆê· í˜• (positive class ê³¼ì†Œ/ê³¼ë‹¤ ì˜ˆì¸¡)
  - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë†’ìŒ

### 3.2 TabTransformer vs FT-Transformer

| ì¸¡ë©´ | TabTransformer | FT-Transformer |
|------|----------------|----------------|
| **í‰ê·  F1-Score** | 0.5146 | 0.4407 |
| **ì„±ëŠ¥ ìš°ìœ„** | âœ… +16.8% | âŒ |
| **íŒŒë¼ë¯¸í„° ìˆ˜** | ~107K | ~224K |
| **íš¨ìœ¨ì„±** | âœ… ë” íš¨ìœ¨ì  | âŒ |
| **í•™ìŠµ ì•ˆì •ì„±** | âœ… ë” ì•ˆì •ì  | âŒ ë¶ˆì•ˆì • |
| **ìˆ˜ë ´ ì†ë„** | âœ… ë¹ ë¦„ (45 epochs) | âŒ ëŠë¦¼ (62-100 epochs) |

**ê²°ë¡ **: TabTransformerê°€ FT-Transformerë³´ë‹¤ tabular dataì— ë” ì í•©

### 3.3 Latent Dimension ì˜í–¥

#### ML ëª¨ë¸
- **4D â†’ 8D**: ì†Œí­ ê°ì†Œ (-0.1%)
- **8D â†’ 12D**: í° í–¥ìƒ (+3.4%)
- **12D â†’ 16D**: ë¯¸ì„¸ í–¥ìƒ (+0.2%)
- **ìµœì **: 16D (ì„±ëŠ¥) ë˜ëŠ” 12D (íš¨ìœ¨)

#### Transformer ëª¨ë¸
- **ì¼ê´€ëœ íŒ¨í„´ ì—†ìŒ**: ì°¨ì› ì¦ê°€ê°€ ì„±ëŠ¥ í–¥ìƒ ë³´ì¥ ì•ˆ í•¨
- **TabTransformer**: 16Dì—ì„œ ìµœê³  (0.5169)
- **FT-Transformer**: 8Dì—ì„œ ìµœê³  (0.4517)
- **í•´ì„**: ëª¨ë¸ ìš©ëŸ‰ ëŒ€ë¹„ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶ˆì•ˆì •

---

## 4. ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ ë¶„ì„

### 4.1 ì„±ëŠ¥ ì°¨ì´ì˜ ì‹¤ë¬´ì  ì˜ë¯¸

**ê°€ì •**:
- ì¼ì¼ ìƒì‚°ëŸ‰: 1,000ê°œ
- ë¶ˆëŸ‰ë¥ : 22.42%
- ë¶ˆëŸ‰í’ˆ ì†ì‹¤: 50,000ì›/ê°œ

#### ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ

| ëª¨ë¸ | F1-Score | ì¼ì¼ íƒì§€ ë¶ˆëŸ‰ | ì¼ì¼ ì†ì‹¤ ì ˆê° | ì—°ê°„ ì ˆê° (ì–µì›) |
|------|----------|----------------|----------------|------------------|
| Baseline (30D) | 0.6422 | 144ê°œ | 7,210,000ì› | 26.3 |
| **Gradient Boosting (16D)** | **0.7043** | **158ê°œ** | **7,915,000ì›** | **28.9** |
| TabTransformer (16D) | 0.5169 | 116ê°œ | 5,800,000ì› | 21.2 |
| FT-Transformer (16D) | 0.4506 | 101ê°œ | 5,050,000ì› | 18.4 |

**ML vs Transformer ì°¨ì´**:
- **ì¼ì¼ ì†ì‹¤ ì ˆê° ì°¨ì´**: 2,115,000ì›
- **ì—°ê°„ ì°¨ì´**: **7.7ì–µì›**
- **ROI ì°¨ì´**: ML ëª¨ë¸ì´ ì••ë„ì  ìš°ìœ„

### 4.2 ìš´ì˜ ë¹„ìš© ê³ ë ¤

| ì¸¡ë©´ | ML Models | Transformers |
|------|-----------|--------------|
| **í•™ìŠµ ì‹œê°„** | 1-5ë¶„ | 30-60ë¶„ |
| **ì¶”ë¡  ì†ë„** | ë§¤ìš° ë¹ ë¦„ | ëŠë¦¼ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ë‚®ìŒ | ë†’ìŒ |
| **ìœ ì§€ë³´ìˆ˜** | ì‰¬ì›€ | ë³µì¡ |
| **í•´ì„ ê°€ëŠ¥ì„±** | ë†’ìŒ | ë‚®ìŒ |
| **ì¬í•™ìŠµ ë¹„ìš©** | ë‚®ìŒ | ë†’ìŒ |

---

## 5. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### 5.1 í•µì‹¬ ê²°ë¡ 

1. **Tree-based ML ëª¨ë¸ì´ Transformerë³´ë‹¤ ìš°ìˆ˜**
   - F1-Score: í‰ê·  24.5% ë†’ìŒ
   - ëª¨ë“  latent dimensionì—ì„œ ì¼ê´€ëœ ìš°ìœ„

2. **TabTransformer > FT-Transformer**
   - Tabular dataì—ëŠ” TabTransformerê°€ ë” ì í•©
   - 16.8% ì„±ëŠ¥ ìš°ìœ„, ë” íš¨ìœ¨ì 

3. **ë°ì´í„°ì…‹ í¬ê¸°ê°€ í•µì‹¬ ìš”ì¸**
   - 7,535 ìƒ˜í”Œì€ transformerì— ë¶€ì¡±
   - Tree-based ëª¨ë¸ì´ ì†Œê·œëª¨ ë°ì´í„°ì— ê°•ì 

4. **ì‹¤ë¬´ ì ìš© ê´€ì **
   - ML ëª¨ë¸: ë¹ ë¥´ê³ , ì•ˆì •ì ì´ê³ , í•´ì„ ê°€ëŠ¥
   - Transformer: ì¶”ê°€ ì´ì  ì—†ìŒ

### 5.2 ìµœì¢… ê¶Œì¥ì‚¬í•­

#### ğŸ† í”„ë¡œë•ì…˜ ë°°í¬ ê¶Œì¥ êµ¬ì„±

**1ìˆœìœ„: Gradient Boosting + 12D Latent (42D Total)**
- F1-Score: 0.7027
- ROC-AUC: 0.9175 (ìµœê³ )
- ì„±ëŠ¥/íš¨ìœ¨ ìµœì  ê· í˜•
- ì—°ê°„ ì ˆê°: 28.7ì–µì›

**2ìˆœìœ„: Gradient Boosting + 16D Latent (46D Total)**
- F1-Score: 0.7043 (ìµœê³ )
- ROC-AUC: 0.9073
- ìµœê³  ì„±ëŠ¥ ì¶”êµ¬ ì‹œ
- ì—°ê°„ ì ˆê°: 28.9ì–µì›

**ë¹„ê¶Œì¥: Transformer ëª¨ë¸**
- í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œëŠ” ì‹¤ìš©ì„± ì—†ìŒ
- ì„±ëŠ¥, íš¨ìœ¨, í•´ì„ì„± ëª¨ë‘ ì—´ìœ„

### 5.3 Transformer ëª¨ë¸ ê°œì„  ë°©í–¥ (í–¥í›„ ì—°êµ¬)

ë§Œì•½ transformerë¥¼ ê°œì„ í•˜ê³ ì í•œë‹¤ë©´:

1. **ë°ì´í„° ì¦ê°•**
   - ìµœì†Œ 50,000+ ìƒ˜í”Œ í™•ë³´
   - SMOTE, ADASYN ë“± ì˜¤ë²„ìƒ˜í”Œë§
   - Data augmentation ê¸°ë²• ì ìš©

2. **ì•„í‚¤í…ì²˜ ìµœì í™”**
   - ë” ì‘ì€ ëª¨ë¸ (íŒŒë¼ë¯¸í„° 50K ì´í•˜)
   - Regularization ê°•í™”
   - Ensemble with ML models

3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - Learning rate ìµœì í™”
   - Dropout ì¡°ì •
   - Loss function ê°œì„  (Focal Loss ë“±)

4. **Pre-training**
   - ìœ ì‚¬ ë„ë©”ì¸ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ
   - Transfer learning ì ìš©

5. **Hybrid ì ‘ê·¼**
   - Transformer features + ML classifier
   - Stacking ensemble

### 5.4 ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

âœ… **ì¦‰ì‹œ ì ìš© ê°€ëŠ¥**:
- [x] Gradient Boosting + 12D/16D Latent
- [x] AutoEncoderë¡œ latent features ìƒì„±
- [x] ê¸°ì¡´ 30D featuresì™€ ê²°í•©

âŒ **ì¶”ê°€ ì—°êµ¬ í•„ìš”**:
- [ ] Transformer ëª¨ë¸ (í˜„ì¬ ì„±ëŠ¥ ë¶€ì¡±)
- [ ] ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ í›„ ì¬í‰ê°€
- [ ] Hybrid ì•„í‚¤í…ì²˜ ì‹¤í—˜

---

## 6. ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### 6.1 ì¬í˜„ì„± ì •ë³´

**í™˜ê²½**:
- Python 3.8+
- PyTorch 2.0+
- scikit-learn 1.3+
- Device: CPU (GPU ì‚¬ìš© ì‹œ ë” ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥)

**íŒŒì¼**:
- `transformer_models.py`: ëª¨ë¸ êµ¬í˜„ ë° í•™ìŠµ ì½”ë“œ
- `best_FTTransformer.pth`: FT-Transformer ê°€ì¤‘ì¹˜
- `best_SimpleTabTransformer.pth`: TabTransformer ê°€ì¤‘ì¹˜
- `transformer_vs_ml_results.csv`: ì „ì²´ ê²°ê³¼
- `transformer_vs_ml_comparison.png`: ì‹œê°í™”

### 6.2 í•™ìŠµ ì‹œê°„

| Model | Latent Dim | Epochs | Time (CPU) |
|-------|------------|--------|------------|
| FT-Transformer | 8D | 100 | ~45ë¶„ |
| TabTransformer | 8D | 45 | ~25ë¶„ |
| FT-Transformer | 12D | 38 | ~20ë¶„ |
| TabTransformer | 12D | 36 | ~20ë¶„ |
| FT-Transformer | 16D | 62 | ~35ë¶„ |
| TabTransformer | 16D | 52 | ~30ë¶„ |

**Total**: ~175ë¶„ (ì•½ 3ì‹œê°„)

### 6.3 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| Model | Parameters | Memory (Training) | Memory (Inference) |
|-------|------------|-------------------|-------------------|
| FT-Transformer | 224K | ~500MB | ~50MB |
| TabTransformer | 107K | ~300MB | ~30MB |
| Gradient Boosting | - | ~100MB | ~10MB |

---

## 7. ì°¸ê³ ë¬¸í—Œ

1. **FT-Transformer**:
   - Gorishniy et al., "Revisiting Deep Learning Models for Tabular Data", NeurIPS 2021
   - https://arxiv.org/abs/2106.11959

2. **TabTransformer**:
   - Huang et al., "TabTransformer: Tabular Data Modeling Using Contextual Embeddings", 2020
   - https://arxiv.org/abs/2012.06678

3. **Tree-based Models for Tabular Data**:
   - Shwartz-Ziv & Armon, "Tabular data: Deep learning is not all you need", 2021
   - https://arxiv.org/abs/2106.03253

---

## ë¶€ë¡: ìƒì„¸ ì‹¤í—˜ ë¡œê·¸

### A.1 FT-Transformer í•™ìŠµ ë¡œê·¸ (8D Latent)

```
Epoch [10/100]
  Train Loss: 1.0591, Val Loss: 1.0409
  F1: 0.3831, Acc: 0.6025, AUC: 0.6204

Epoch [90/100]
  Train Loss: 1.0043, Val Loss: 0.9822
  F1: 0.4517, Acc: 0.6344, AUC: 0.6954

ìµœì¢… ì„±ëŠ¥: F1: 0.4517, Acc: 0.6344, AUC: 0.6954
```

### A.2 TabTransformer í•™ìŠµ ë¡œê·¸ (8D Latent)

```
Epoch [10/100]
  Train Loss: 0.9411, Val Loss: 0.9094
  F1: 0.4880, Acc: 0.6045, AUC: 0.7514

Epoch [30/100]
  Train Loss: 0.8951, Val Loss: 0.9043
  F1: 0.5104, Acc: 0.7034, AUC: 0.7574

Early stopping at epoch 45
ìµœì¢… ì„±ëŠ¥: F1: 0.5104, Acc: 0.7034, AUC: 0.7574
```

---

**ë³´ê³ ì„œ ì‘ì„±**: Kiro AI Assistant  
**ê²€ì¦**: Ablation Study ê¸°ë°˜ ì‹¤í—˜ ê²°ê³¼  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-15
